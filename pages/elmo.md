---
title: (ELMo) Deep Contextualized Word Representations
tags: [architecture]
keywords: elmo, lstm, peters, neumann, iyyer, gardner, clard, sentence, embeddings, peters et al, 2018, allen institute, allen, allen school, university of washington, washington
last_updated: July 2021
summary: "Sentence Embeddings made with two layers of bidirectional LSTMs."
sidebar: home_sidebar
permalink: elmo.html
---

[PDF](https://arxiv.org/pdf/1802.05365.pdf)

[Papers with Code](https://paperswithcode.com/paper/deep-contextualized-word-representations)


> Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, Zettlemoyer L. Deep contextualized word representations. arXiv preprint arXiv:1802.05365. 2018 Feb 15.


## Summary


## Architecture Details

### Tokenization


### Embeddings for multi-segment tasks


### Positional Encoding


### Attention Mechanism


## Training Details

### Pre-training Data

### Pre-training Method



### Finetuning Data

### Finetuning Method


## Evaluation

### Evaluation Datasets

### Evaluation Results


## Author's Conclusions