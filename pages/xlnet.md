---
title: "(XLNET) XLNET: Generalized Autoregressize Pretraining for Language Understanding"
tags: [architecture]
keywords: xlnet, transformer, attention, yang, dai, carbonell, yang et al, 2019, google, brain, carnegie, mellon, google brain
last_updated: July 2021
summary: "Paper finds that Next Sentence Prediction (NSP) from [BERT] does not necessarily improve performance."
sidebar: home_sidebar
permalink: xlnet.html
---

[PDF](https://arxiv.org/pdf/1906.08237.pdf)

[Papers with Code](https://paperswithcode.com/paper/xlnet-generalized-autoregressive-pretraining)

> Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov R, Le QV. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237. 2019 Jun 19.

## Summary


## Architecture Details

### Tokenization


### Embeddings for multi-segment tasks


### Positional Encoding


### Attention Mechanism


## Training Details

### Pre-training Data

### Pre-training Method



### Finetuning Data

### Finetuning Method


## Evaluation

### Evaluation Datasets

### Evaluation Results


## Author's Conclusions