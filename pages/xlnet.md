---
title: "(XLNET) XLNET: Generalized Autoregressize Pretraining for Language Understanding"
tags: [architecture]
keywords: xlnet, transformer, attention, yang, dai, carbonell
last_updated: July 2021
summary: "Paper finds that Next Sentence Prediction (NSP) from [BERT] does not necessarily improve performance."
sidebar: home_sidebar
permalink: xlnet.html
---

[PDF](https://arxiv.org/pdf/1906.08237.pdf)

[Papers with Code](https://paperswithcode.com/paper/xlnet-generalized-autoregressive-pretraining)

> Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov R, Le QV. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237. 2019 Jun 19.

