# Chaloner's Transformer Wiki

There are plenty of Transformer methods out there, and it's difficult to keep everything in my head. I'm writing everything out for my future self as well as any travellers who find themselves here!

## Transformer Models

In Progress:
* [Transformer](https://github.com/AlexChaloner/Transformer-Model-Summaries/wiki/(Transformer)-Attention-Is-All-You-Need)
* [ELMo]((ELMo)-Deep-contextualized-word-representations) (Not a Transformer, but important)
* [BERT](https://github.com/AlexChaloner/Transformer-Model-Summaries/wiki/(BERT)-BERT:-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding)
* [ALBERT](https://github.com/AlexChaloner/Transformer-Model-Summaries/wiki/(ALBERT)-ALBERT:-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations)

Planned:
* RoBERTa
* DeBERTa
* T5
* GPT-1
* GPT-2
* GPT-3
* BART


## Overview Pages

Planned:
* Pretraining FLOPs vs finetuning FLOPs vs model size vs Benchmark Performance
* Attention Mechanisms
* Pretraining Methods
* Datasets
* Tokenization Methods
* Positional Encoding Methods
* Conflicting findings from review papers
* Timeline

